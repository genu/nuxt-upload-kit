---
title: Azure Data Lake
description: Upload files to Azure Data Lake Storage Gen2.
navigation:
  icon: i-simple-icons-microsoftazure
---

# Azure Data Lake Storage

The `PluginAzureDataLake` adapter uploads files to Azure Data Lake Storage Gen2 using SAS (Shared Access Signature) authentication.

## Installation

Install the Azure SDK:

```bash [Terminal]
pnpm add @azure/storage-file-datalake
```

## Usage

```ts
import { PluginAzureDataLake } from "nuxt-upload-kit/providers/azure-datalake"

const uploader = useUploadKit({
  storage: PluginAzureDataLake({
    sasURL:
      "https://mystorageaccount.blob.core.windows.net/mycontainer?sv=2022-11-02&ss=b&srt=co&sp=rwdlaciytfx&se=2024-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=...",
    path: "uploads/images",
  }),
})
```

::prose-callout{type="info" title="Separate Import"}
The Azure adapter is imported from `nuxt-upload-kit/providers/azure-datalake` to avoid bundling the Azure SDK for users who don't need it.
::

## Options

| Option                | Type                                      | Default | Description                               |
| --------------------- | ----------------------------------------- | ------- | ----------------------------------------- |
| `sasURL`              | `string`                                  | -       | Static SAS URL for the container          |
| `getSASUrl`           | `(storageKey: string) => Promise<string>` | -       | Function to dynamically fetch SAS URL     |
| `path`                | `string`                                  | -       | Subdirectory path within the container    |
| `metadata`            | `Record<string, string>`                  | -       | Custom metadata for uploaded files        |
| `pathHttpHeaders`     | `object`                                  | -       | Custom HTTP headers                       |
| `autoCreateDirectory` | `boolean`                                 | `true`  | Auto-create directory if it doesn't exist |

## Authentication

### Static SAS URL

For simple use cases, provide a static SAS URL:

```ts
PluginAzureDataLake({
  sasURL: "https://mystorageaccount.blob.core.windows.net/container?sv=...",
})
```

### Dynamic SAS URL

For production, fetch SAS tokens dynamically:

```ts
PluginAzureDataLake({
  getSASUrl: async (storageKey) => {
    const response = await fetch("/api/storage/sas-token", {
      method: "POST",
      body: JSON.stringify({ storageKey }),
    })
    const { url } = await response.json()
    return url
  },
})
```

The adapter auto-detects the SAS type from the `sr` (signed resource) parameter:

| SAS Type | Parameter | Behavior |
| -------- | --------- | -------- |
| Directory (`sr=d`) | Cached and reused for batch uploads |
| File (`sr=b`) | Called per file for granular access control |

The adapter also:

- Checks token expiration before operations (5-minute buffer)
- Refreshes expired tokens automatically
- Deduplicates concurrent refresh requests (directory mode only)

## Creating a SAS Token

Generate SAS tokens on your backend:

```ts [server/api/storage/sas-token.get.ts]
import { generateBlobSASQueryParameters, BlobSASPermissions } from "@azure/storage-blob"
import { StorageSharedKeyCredential } from "@azure/storage-blob"

export default defineEventHandler(async () => {
  const accountName = process.env.AZURE_STORAGE_ACCOUNT!
  const accountKey = process.env.AZURE_STORAGE_KEY!
  const containerName = "uploads"

  const credential = new StorageSharedKeyCredential(accountName, accountKey)

  const startsOn = new Date()
  const expiresOn = new Date(startsOn.getTime() + 60 * 60 * 1000) // 1 hour

  const sasToken = generateBlobSASQueryParameters(
    {
      containerName,
      permissions: BlobSASPermissions.parse("racwd"), // read, add, create, write, delete
      startsOn,
      expiresOn,
    },
    credential,
  ).toString()

  return {
    url: `https://${accountName}.blob.core.windows.net/${containerName}?${sasToken}`,
  }
})
```

## Directory Structure

Organize uploads with the `path` option:

```ts
// All files go to: container/uploads/2024/images/
PluginAzureDataLake({
  sasURL: "...",
  path: "uploads/2024/images",
})
```

### Dynamic Paths

Create dynamic paths based on user or date:

```ts
const userId = useAuth().user.id
const date = new Date().toISOString().split("T")[0]

const uploader = useUploadKit({
  storage: PluginAzureDataLake({
    getSASUrl: () => fetchSasUrl(),
    path: `users/${userId}/${date}`,
  }),
})
```

## Custom Metadata

Attach metadata to uploaded files:

```ts
PluginAzureDataLake({
  sasURL: "...",
  metadata: {
    uploadedBy: "user123",
    application: "my-app",
    version: "1.0.0",
  },
})
```

The adapter automatically adds:

- `mimeType` - File MIME type
- `size` - File size in bytes
- `originalName` - Original filename

## HTTP Headers

Set custom headers (e.g., for caching):

```ts
PluginAzureDataLake({
  sasURL: "...",
  pathHttpHeaders: {
    cacheControl: "public, max-age=31536000",
    contentDisposition: "inline",
  },
})
```

::prose-tip
The `contentType` header is automatically set from the file's MIME type.
::

## Upload Result

After successful upload, `file.uploadResult` contains:

```ts
{
  url: 'https://mystorageaccount.blob.core.windows.net/container/path/file.jpg',
  storageKey: 'path/file.jpg'
}
```

Access it after upload:

```ts
uploader.on("upload:complete", (files) => {
  files.forEach((file) => {
    console.log("Uploaded to:", file.uploadResult.url)
    console.log("Storage key:", file.uploadResult.storageKey)
  })
})
```

## Loading Existing Files

Load previously uploaded files:

```ts
// The adapter's getRemoteFile hook fetches metadata from Azure
await uploader.initializeExistingFiles([{ id: "path/to/file1.jpg" }, { id: "path/to/file2.png" }])
```

## Deleting Files

When you call `removeFile()`, the adapter automatically deletes from Azure:

```ts
// This deletes from Azure and removes from the local list
await uploader.removeFile(file.id)
```

To only remove locally without deleting from Azure:

```ts
// Filter the files array directly
uploader.files.value = uploader.files.value.filter((f) => f.id !== file.id)
```

## Troubleshooting

### "AuthorizationPermissionMismatch"

Your SAS token doesn't have required permissions. Ensure it includes:

- `r` - Read (for getRemoteFile)
- `a` - Add
- `c` - Create (for directory creation)
- `w` - Write
- `d` - Delete (for remove)

### "ContainerNotFound"

The container in your SAS URL doesn't exist. Create it in Azure Portal or via Azure CLI:

```bash
az storage container create --name mycontainer --account-name mystorageaccount
```

### Directory Creation Fails

If your SAS token only has blob-level permissions, disable auto-create:

```ts
PluginAzureDataLake({
  sasURL: "...",
  autoCreateDirectory: false,
})
```
