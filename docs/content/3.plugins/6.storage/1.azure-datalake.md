---
title: Azure Data Lake
description: Upload files to Azure Data Lake Storage Gen2.
navigation:
  icon: i-simple-icons-microsoftazure
---

# Azure Data Lake Storage

The `PluginAzureDataLake` plugin uploads files to Azure Data Lake Storage Gen2 using SAS (Shared Access Signature) authentication.

## Installation

Install the Azure SDK:

```bash [Terminal]
pnpm add @azure/storage-file-datalake
```

## Usage

```ts
import { PluginAzureDataLake } from "nuxt-upload-kit"

const uploader = useUploadKit({
  storage: PluginAzureDataLake({
    sasURL:
      "https://mystorageaccount.blob.core.windows.net/mycontainer?sv=2022-11-02&ss=b&srt=co&sp=rwdlaciytfx&se=2024-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=...",
    path: "uploads/images",
  }),
})
```

## Options

| Option                | Type                     | Default | Description                               |
| --------------------- | ------------------------ | ------- | ----------------------------------------- |
| `sasURL`              | `string`                 | -       | Static SAS URL for the container          |
| `getSASUrl`           | `() => Promise<string>`  | -       | Function to dynamically fetch SAS URL     |
| `path`                | `string`                 | -       | Subdirectory path within the container    |
| `metadata`            | `Record<string, string>` | -       | Custom metadata for uploaded files        |
| `pathHttpHeaders`     | `object`                 | -       | Custom HTTP headers                       |
| `autoCreateDirectory` | `boolean`                | `true`  | Auto-create directory if it doesn't exist |
| `retries`             | `number`                 | `3`     | Number of retry attempts                  |
| `retryDelay`          | `number`                 | `1000`  | Initial retry delay in ms                 |

## Authentication

### Static SAS URL

For simple use cases, provide a static SAS URL:

```ts
PluginAzureDataLake({
  sasURL: "https://mystorageaccount.blob.core.windows.net/container?sv=...",
})
```

### Dynamic SAS URL

For production, fetch SAS tokens dynamically to handle expiration:

```ts
PluginAzureDataLake({
  getSASUrl: async () => {
    const response = await fetch("/api/storage/sas-token")
    const { url } = await response.json()
    return url
  },
})
```

The plugin automatically:

- Checks token expiration before operations
- Refreshes expired tokens
- Handles concurrent refresh requests

## Creating a SAS Token

Generate SAS tokens on your backend:

```ts [server/api/storage/sas-token.get.ts]
import { generateBlobSASQueryParameters, BlobSASPermissions } from "@azure/storage-blob"
import { StorageSharedKeyCredential } from "@azure/storage-blob"

export default defineEventHandler(async () => {
  const accountName = process.env.AZURE_STORAGE_ACCOUNT!
  const accountKey = process.env.AZURE_STORAGE_KEY!
  const containerName = "uploads"

  const credential = new StorageSharedKeyCredential(accountName, accountKey)

  const startsOn = new Date()
  const expiresOn = new Date(startsOn.getTime() + 60 * 60 * 1000) // 1 hour

  const sasToken = generateBlobSASQueryParameters(
    {
      containerName,
      permissions: BlobSASPermissions.parse("racwd"), // read, add, create, write, delete
      startsOn,
      expiresOn,
    },
    credential,
  ).toString()

  return {
    url: `https://${accountName}.blob.core.windows.net/${containerName}?${sasToken}`,
  }
})
```

## Directory Structure

Organize uploads with the `path` option:

```ts
// All files go to: container/uploads/2024/images/
PluginAzureDataLake({
  sasURL: "...",
  path: "uploads/2024/images",
})
```

### Dynamic Paths

Create dynamic paths based on user or date:

```ts
const userId = useAuth().user.id
const date = new Date().toISOString().split("T")[0]

const uploader = useUploadKit({
  storage: PluginAzureDataLake({
    getSASUrl: () => fetchSasUrl(),
    path: `users/${userId}/${date}`,
  }),
})
```

## Custom Metadata

Attach metadata to uploaded files:

```ts
PluginAzureDataLake({
  sasURL: "...",
  metadata: {
    uploadedBy: "user123",
    application: "my-app",
    version: "1.0.0",
  },
})
```

The plugin automatically adds:

- `mimeType` - File MIME type
- `size` - File size in bytes
- `originalName` - Original filename

## HTTP Headers

Set custom headers (e.g., for caching):

```ts
PluginAzureDataLake({
  sasURL: "...",
  pathHttpHeaders: {
    cacheControl: "public, max-age=31536000",
    contentDisposition: "inline",
  },
})
```

::prose-tip
The `contentType` header is automatically set from the file's MIME type.
::

## Error Handling & Retries

The plugin includes built-in retry logic with exponential backoff:

```ts
PluginAzureDataLake({
  sasURL: "...",
  retries: 5, // Try up to 5 times
  retryDelay: 2000, // Start with 2s delay, doubles each retry
})
```

Retry sequence: 2s → 4s → 8s → 16s → fail

## Upload Result

After successful upload, `file.uploadResult` contains:

```ts
{
  url: 'https://mystorageaccount.blob.core.windows.net/container/path/file.jpg',
  blobPath: 'path/file.jpg'
}
```

Access it after upload:

```ts
uploader.on("upload:complete", (files) => {
  files.forEach((file) => {
    console.log("Uploaded to:", file.uploadResult.url)
    console.log("Blob path:", file.uploadResult.blobPath)
  })
})
```

## Loading Existing Files

Load previously uploaded files:

```ts
// The plugin's getRemoteFile hook fetches metadata from Azure
await uploader.initializeExistingFiles([{ id: "path/to/file1.jpg" }, { id: "path/to/file2.png" }])
```

## Deleting Files

When you call `removeFile()`, the plugin automatically deletes from Azure:

```ts
// This deletes from Azure and removes from the local list
await uploader.removeFile(file.id)
```

To only remove locally without deleting from Azure:

```ts
// Filter the files array directly
uploader.files.value = uploader.files.value.filter((f) => f.id !== file.id)
```

## Troubleshooting

### "AuthorizationPermissionMismatch"

Your SAS token doesn't have required permissions. Ensure it includes:

- `r` - Read (for getRemoteFile)
- `a` - Add
- `c` - Create (for directory creation)
- `w` - Write
- `d` - Delete (for remove)

### "ContainerNotFound"

The container in your SAS URL doesn't exist. Create it in Azure Portal or via Azure CLI:

```bash
az storage container create --name mycontainer --account-name mystorageaccount
```

### Directory Creation Fails

If your SAS token only has blob-level permissions, disable auto-create:

```ts
PluginAzureDataLake({
  sasURL: "...",
  autoCreateDirectory: false,
})
```
